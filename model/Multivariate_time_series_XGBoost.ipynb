{
 "cells": [
  {
   "attachments": {
    "ds4a_colombia.svg": {
     "image/svg+xml": [
      "<svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" viewBox="0 0 308.55 47.71"><defs><style>.cls-1{fill:none;}.cls-2{clip-path:url(#clip-path);}.cls-3{fill:#699dd3;}.cls-4{fill:#3a3749;}.cls-5{clip-path:url(#clip-path-2);}.cls-6{fill:#fff;}.cls-7{fill:#ececec;}</style><clipPath id="clip-path" transform="translate(0 0)"><rect class="cls-1" width="308.55" height="47.71"/></clipPath><clipPath id="clip-path-2" transform="translate(0 0)"><rect class="cls-1" width="308.55" height="47.71"/></clipPath></defs><g id="Layer_2" data-name="Layer 2"><g id="Layer_1-2" data-name="Layer 1"><g id="COLOMBIA_MAIN_SANS_TAG" data-name="COLOMBIA MAIN SANS TAG"><g class="cls-2"><rect class="cls-3" x="26.67" width="43.54" height="47.71"/><rect class="cls-4" x="73.39" width="235.16" height="47.71"/><g class="cls-5"><path class="cls-4" d="M19.67,4.51a2.81,2.81,0,0,1-5.61-.11,2.81,2.81,0,0,1,5.61.11" transform="translate(0 0)"/><path class="cls-4" d="M12.78,12.33A2.8,2.8,0,1,1,10,9.53a2.79,2.79,0,0,1,2.75,2.8" transform="translate(0 0)"/><path class="cls-4" d="M5.61,20.42A2.79,2.79,0,0,1,2.75,23.1a2.74,2.74,0,1,1,.11-5.48,2.77,2.77,0,0,1,2.75,2.8" transform="translate(0 0)"/><path class="cls-6" d="M35.29,8.69l5.17,0A11.69,11.69,0,0,1,43.32,9a7.54,7.54,0,0,1,2.6,1.15,6.06,6.06,0,0,1,1.89,2.12,6.77,6.77,0,0,1,.75,3.28,6.44,6.44,0,0,1-.69,3.14A6.55,6.55,0,0,1,46,20.84a8,8,0,0,1-2.58,1.27,10.94,10.94,0,0,1-2.86.42l-5.16,0Zm4.85,11.69a9.86,9.86,0,0,0,2-.24A5.55,5.55,0,0,0,44,19.36a4.27,4.27,0,0,0,1.32-1.47,4.72,4.72,0,0,0,.49-2.34,5.08,5.08,0,0,0-.53-2.43,4.12,4.12,0,0,0-1.34-1.46A5.14,5.14,0,0,0,42.08,11a11.41,11.41,0,0,0-2-.18l-2.15,0,.07,9.6Z" transform="translate(0 0)"/><path class="cls-6" d="M57.77,11.61a3.11,3.11,0,0,0-1.15-.83,3.45,3.45,0,0,0-1.49-.32,3.87,3.87,0,0,0-.89.11,2.7,2.7,0,0,0-.8.33,1.92,1.92,0,0,0-.59.58,1.66,1.66,0,0,0,0,1.65,1.87,1.87,0,0,0,.56.56,4,4,0,0,0,.86.39c.34.12.71.23,1.12.34s.93.3,1.42.47a5.56,5.56,0,0,1,1.37.71,3.7,3.7,0,0,1,1,1.1,3.31,3.31,0,0,1,.4,1.68,4.05,4.05,0,0,1-.41,1.92,3.81,3.81,0,0,1-1.12,1.36,4.73,4.73,0,0,1-1.65.82,7.21,7.21,0,0,1-2,.29,7.85,7.85,0,0,1-2.73-.47,5.25,5.25,0,0,1-2.19-1.44l1.8-1.63a3.73,3.73,0,0,0,1.42,1.09,4.25,4.25,0,0,0,1.73.39,4.18,4.18,0,0,0,.91-.11,2.51,2.51,0,0,0,.83-.37,2,2,0,0,0,.6-.63,1.69,1.69,0,0,0,.23-.93,1.52,1.52,0,0,0-.27-.9,2.51,2.51,0,0,0-.71-.61A5,5,0,0,0,55,16.72l-1.28-.4a11,11,0,0,1-1.29-.45,4.6,4.6,0,0,1-1.17-.7,3.52,3.52,0,0,1-.85-1.09,3.45,3.45,0,0,1-.33-1.6,3.48,3.48,0,0,1,.43-1.8,3.94,3.94,0,0,1,1.19-1.25,5.28,5.28,0,0,1,1.67-.74,7.65,7.65,0,0,1,1.91-.26,7.54,7.54,0,0,1,2.21.35,5.45,5.45,0,0,1,2,1.09Z" transform="translate(0 0)"/><path class="cls-6" d="M53.58,24.69l2.35,0,6.69,14.49-3.1,0-1.45-3.31-6.62.05L50.1,39.25l-3,0Zm3.52,9-2.4-5.9-2.34,5.93Z" transform="translate(0 0)"/><path class="cls-6" d="M41.58,36.25H34.26V34.07L41,24.7h3.22v9.43h2.22v2.12H44.21v3H41.58Zm0-8.65h0L37,34.13h4.55Z" transform="translate(0 0)"/><path class="cls-7" d="M101.21,30.66a4.51,4.51,0,0,1-2.34.57,4.73,4.73,0,0,1-2.43-.63,5.39,5.39,0,0,1-1.81-1.75,8.81,8.81,0,0,1-1.13-2.64,13.24,13.24,0,0,1-.39-3.31,13,13,0,0,1,.4-3.33,8.61,8.61,0,0,1,1.15-2.64,5.65,5.65,0,0,1,1.81-1.74,4.68,4.68,0,0,1,2.4-.63,5.5,5.5,0,0,1,2.28.47,4.19,4.19,0,0,1,1.69,1.44l-1.28,1.37a3.33,3.33,0,0,0-1.17-1.06A3,3,0,0,0,99,16.43a2.68,2.68,0,0,0-1.63.51,4.08,4.08,0,0,0-1.13,1.4,7.59,7.59,0,0,0-.68,2.05,13.38,13.38,0,0,0-.23,2.51,13.17,13.17,0,0,0,.23,2.49,7.56,7.56,0,0,0,.69,2.06,3.84,3.84,0,0,0,1.16,1.4,2.65,2.65,0,0,0,1.64.52,2.56,2.56,0,0,0,1.5-.45,3.54,3.54,0,0,0,1-1.1l1.24,1.32a4.72,4.72,0,0,1-1.59,1.52" transform="translate(0 0)"/><path class="cls-7" d="M123,22.88a14.38,14.38,0,0,1-.38,3.4,8.34,8.34,0,0,1-1.11,2.64,5.45,5.45,0,0,1-1.74,1.71,4.52,4.52,0,0,1-2.33.6,4.58,4.58,0,0,1-2.34-.6,5.41,5.41,0,0,1-1.76-1.71,8.56,8.56,0,0,1-1.1-2.64,14.38,14.38,0,0,1-.38-3.4,14.43,14.43,0,0,1,.37-3.41,8.16,8.16,0,0,1,1.08-2.63A5.1,5.1,0,0,1,115,15.16a4.7,4.7,0,0,1,2.38-.6,4.52,4.52,0,0,1,2.33.6,5.25,5.25,0,0,1,1.74,1.68,8.18,8.18,0,0,1,1.11,2.63,14.41,14.41,0,0,1,.38,3.41m-2.18,0a14.56,14.56,0,0,0-.22-2.54,7.88,7.88,0,0,0-.65-2.07,3.94,3.94,0,0,0-1-1.38,2.26,2.26,0,0,0-1.46-.51,2.32,2.32,0,0,0-1.49.51,3.87,3.87,0,0,0-1.07,1.38,7.89,7.89,0,0,0-.63,2.07,15.41,15.41,0,0,0,0,5.07,7.89,7.89,0,0,0,.63,2.07,3.78,3.78,0,0,0,1.07,1.38,2.32,2.32,0,0,0,1.49.51,2.26,2.26,0,0,0,1.46-.51,3.85,3.85,0,0,0,1-1.38,7.88,7.88,0,0,0,.65-2.07,14.53,14.53,0,0,0,.22-2.53" transform="translate(0 0)"/><polygon class="cls-7" points="133.66 30.85 133.66 14.95 135.72 14.95 135.72 28.96 140.31 28.96 140.31 30.85 133.66 30.85"/><path class="cls-7" d="M159.78,22.88a13.88,13.88,0,0,1-.38,3.4,8.56,8.56,0,0,1-1.1,2.64,5.45,5.45,0,0,1-1.74,1.71,4.84,4.84,0,0,1-4.67,0,5.41,5.41,0,0,1-1.76-1.71,8.56,8.56,0,0,1-1.1-2.64,13.88,13.88,0,0,1-.38-3.4,14.43,14.43,0,0,1,.37-3.41,8.38,8.38,0,0,1,1.07-2.63,5.12,5.12,0,0,1,1.76-1.68,4.7,4.7,0,0,1,2.38-.6,4.52,4.52,0,0,1,2.33.6,5.25,5.25,0,0,1,1.74,1.68,8.4,8.4,0,0,1,1.1,2.63,13.91,13.91,0,0,1,.38,3.41m-2.18,0a13.65,13.65,0,0,0-.22-2.54,7.88,7.88,0,0,0-.64-2.07,3.94,3.94,0,0,0-1.05-1.38,2.28,2.28,0,0,0-1.46-.51,2.3,2.3,0,0,0-1.49.51,3.87,3.87,0,0,0-1.07,1.38,7.21,7.21,0,0,0-.63,2.07,15.41,15.41,0,0,0,0,5.07,7.21,7.21,0,0,0,.63,2.07,3.78,3.78,0,0,0,1.07,1.38,2.3,2.3,0,0,0,1.49.51,2.28,2.28,0,0,0,1.46-.51,3.85,3.85,0,0,0,1.05-1.38,7.88,7.88,0,0,0,.64-2.07,13.62,13.62,0,0,0,.22-2.53" transform="translate(0 0)"/><polygon class="cls-7" points="181.59 30.85 181.59 17.69 181.52 17.69 177.9 30.85 176.04 30.85 172.46 17.69 172.39 17.69 172.39 30.85 170.48 30.85 170.48 14.95 173.52 14.95 176.99 27.57 177.08 27.57 180.49 14.95 183.66 14.95 183.66 30.85 181.59 30.85"/><path class="cls-7" d="M201.06,22.49a3.41,3.41,0,0,1,.93.36,3.08,3.08,0,0,1,.91.76,3.8,3.8,0,0,1,.69,1.19,4.67,4.67,0,0,1,.28,1.67,4.6,4.6,0,0,1-.42,2.07A4,4,0,0,1,201,30.63a5.43,5.43,0,0,1-1.52.22h-4.39V15H199a6.93,6.93,0,0,1,1.46.16,4.05,4.05,0,0,1,1.41.62,3.57,3.57,0,0,1,1.06,1.21,4.13,4.13,0,0,1,.41,1.94,4.69,4.69,0,0,1-.18,1.37,3.73,3.73,0,0,1-.5,1,2.9,2.9,0,0,1-.71.73,3.8,3.8,0,0,1-.84.45Zm.3-3.31a2.75,2.75,0,0,0-.23-1.21,2.13,2.13,0,0,0-1.43-1.22,4,4,0,0,0-1-.12H197v5.14h1.77a3.09,3.09,0,0,0,.93-.14,2.62,2.62,0,0,0,.84-.45,2.32,2.32,0,0,0,.59-.8,2.76,2.76,0,0,0,.23-1.2m.44,7.17a3.38,3.38,0,0,0-.26-1.39,2.87,2.87,0,0,0-.68-.93,2.63,2.63,0,0,0-.93-.5,3.38,3.38,0,0,0-1-.16H197v5.8h2a3.6,3.6,0,0,0,1.09-.17,2.69,2.69,0,0,0,.89-.51,2.26,2.26,0,0,0,.6-.87,3.24,3.24,0,0,0,.22-1.27" transform="translate(0 0)"/><rect class="cls-7" x="214.36" y="14.94" width="2.07" height="15.91"/><path class="cls-7" d="M234.88,30.85,234,27.12h-4.7l-.94,3.73h-2.09L230.31,15h2.74l4,15.9Zm-3.19-14h-.09l-2.07,8.7h4.16Z" transform="translate(0 0)"/></g></g></g></g></g></svg>"
     ]
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![ds4a_colombia.svg](attachment:ds4a_colombia.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Impacto de la deforestaci√≥n en el regimen de caudales de los rios en Colombia (TEAM 28)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multivariate time series forecasting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sources :\n",
    "\n",
    "https://towardsdatascience.com/vector-autoregressions-vector-error-correction-multivariate-model-a69daf6ab618\n",
    "\n",
    "https://towardsdatascience.com/pairs-trading-with-cryptocurrencies-e79b4a00b015"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.tsa.vector_ar as var\n",
    "import sklearn.metrics as skm\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import figure\n",
    "figure(num = None, figsize = (15, 12), dpi = 80, facecolor = 'w', edgecolor = 'k')\n",
    "plt.rcParams.update({'font.size': 16, 'figure.figsize': (15, 10), \n",
    "                     'figure.max_open_warning': 200})\n",
    "\n",
    "# machine learning: XGB\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV\n",
    "from xgboost.sklearn import XGBRegressor # wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(plt.rcParams.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "macrodata = pd.read_csv('../data/matrix/matrix_consol_v2.zip')\n",
    "\n",
    "macrodata.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mcs = macrodata['mc'].unique()\n",
    "mcs.sort()\n",
    "\n",
    "print(mcs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split data for machine learning algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_df = macrodata.copy()\n",
    "\n",
    "data_train = pd.DataFrame()\n",
    "data_test = pd.DataFrame()\n",
    "\n",
    "for i in mcs:\n",
    "    #train, test = train_test_split(temp_df[temp_df['mc'] == i], test_size = 0.2)\n",
    "    nobs = 24 # 10% de 240\n",
    "    train, test = temp_df[temp_df['mc'] == i].iloc[0:-nobs], temp_df[temp_df['mc'] == i].iloc[-nobs:]\n",
    "    data_train = pd.concat([data_train, train], axis = 0)\n",
    "    data_test = pd.concat([data_test, test], axis = 0)\n",
    "\n",
    "print('Total data')\n",
    "print('----------')\n",
    "print(macrodata.shape)\n",
    "print(macrodata.dtypes)\n",
    "print()\n",
    "print('data_train')\n",
    "print('----------')\n",
    "print(data_train.shape)\n",
    "print(data_train.dtypes)\n",
    "print()\n",
    "print('data_test')\n",
    "print('---------')\n",
    "print(data_test.shape)\n",
    "print(data_test.dtypes)\n",
    "\n",
    "temp_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_test.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates = pd.DataFrame(pd.date_range('2018-01-01','2019-12-31' , freq='1M') - \n",
    "             pd.offsets.MonthBegin(1))\n",
    "dates.columns = ['date']\n",
    "\n",
    "dates['year'] = pd.DatetimeIndex(dates['date']).year\n",
    "dates['month'] = pd.DatetimeIndex(dates['date']).month\n",
    "\n",
    "dates.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[XGBoost](https://github.com/dmlc/xgboost/blob/master/doc/model.md) is an implementation of Gradient Boosted Decision trees designed for speed and performance. Its more suitable name is a as [regularized Gradient Boosting](http://datascience.la/xgboost-workshop-and-meetup-talk-with-tianqi-chen/), as it uses a more regularized model formalization to control over-fitting.\n",
    "\n",
    "Additional advantages of this algorythm are:\n",
    "- Automated missing values handling: XGB uses a \"learned\" default direction for the missing values. \"Learned\" means learned in the tree construction process by choosing the best direction that optimizes the training loss.\n",
    "- Interactive feature analysis (yet implemented only in R): plots the structure of decision trees with splits and leaves.\n",
    "- Feature importance analysis: a sorted barplot of the most significant variables.\n",
    "\n",
    "<div class = \"alert alert-block alert-info\"> As we already saw in the previos section our data is higly seasonal and not random (dependent). Therefore, before fitting any models we need to \"smooth\" target variable Sales. The typical preprocessing step is to log transform the data in question. Once we perform the forecasting we will unwind log transformations in reverse order. </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training\n",
    "\n",
    "**Approach**\n",
    "\n",
    "1. Split train data to train and test set to evaluate the model.\n",
    "2. Set eta to a relatively high value (e.g. 0.05 ~ 0.1), num_round to 300 ~ 500\n",
    "3. Use grid search to find the best combination of additional parameters.\n",
    "4. Lower eta until we reach the optimum.\n",
    "5. Use the validation set as watchlist to retrain the model with the best parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split into training and evaluation sets\n",
    "\n",
    "# v_flow_mean\n",
    "# v_loss_cover\n",
    "# v_rainfall_total\n",
    "# v_temperature_mean\n",
    "# v_flow_mean_log\n",
    "# v_loss_cover_log\n",
    "# v_rainfall_total_log\n",
    "# v_flow_mean_log_diff\n",
    "# v_loss_cover_log_diff\n",
    "# v_rainfall_total_log_diff\n",
    "\n",
    "# predictors = [x for x in train_store.columns if x not in ['Customers', 'Sales', 'SalePerCustomer']]\n",
    "# y = np.log(train_store.Sales) # log transformation of Sales\n",
    "# X = train_store\n",
    "\n",
    "# # split the data into train/test set\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "#                                                     test_size = 0.1, # 30% for the evaluation set\n",
    "#                                                     random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train['v_loss_cover_porc'] = data_train['v_loss_cover'] * 1000\n",
    "\n",
    "data_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test['v_loss_cover_porc'] = data_test['v_loss_cover'] * 1000\n",
    "\n",
    "data_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictors = ['v_loss_cover_porc', 'v_rainfall_total']\n",
    "# predictors = ['v_loss_cover', 'v_rainfall_total']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluation metric: rmspe\n",
    "# Root Mean Square Percentage Error\n",
    "# code chunk shared at Kaggle\n",
    "\n",
    "def rmspe(y, yhat):\n",
    "    return np.sqrt(np.mean((yhat / y-1) ** 2))\n",
    "\n",
    "def rmspe_xg(yhat, y):\n",
    "    y = np.expm1(y.get_label())\n",
    "    yhat = np.expm1(yhat)\n",
    "    return \"rmspe\", rmspe(y, yhat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tuning Parameters** \n",
    "\n",
    "- eta: Step size used in updating weights. Lower value means slower training but better convergence.\n",
    "- num_round: Total number of iterations.\n",
    "- subsample: The ratio of training data used in each iteration; combat overfitting. Should be configured in the range of 30% to 80% of the training dataset, and compared to a value of 100% for no sampling.\n",
    "- colsample_bytree: The ratio of features used in each iteration, default 1.\n",
    "- max_depth: The maximum depth of each tree. If we do not limit max depth, gradient boosting would eventually overfit.\n",
    "- early_stopping_rounds: If there's no increase in validation score for a given number of iterations, the algorithm will stop early, also combats overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGB with xgboost library (First simplified version just with train - Version 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# base parameters\n",
    "# params = {\n",
    "#     'booster': 'gbtree', \n",
    "#     'objective': 'reg:squarederror', # regression task\n",
    "#     'subsample': 0.8, # 80% of data to grow trees and prevent overfitting\n",
    "#     'colsample_bytree': 0.85, # % of features used\n",
    "#     'eta': 0.08, \n",
    "#     'max_depth': 10, \n",
    "#     'seed': 42} # for reproducible results\n",
    "\n",
    "params = {\n",
    "    'booster': 'gbtree', \n",
    "    'objective': 'reg:squarederror', # regression task\n",
    "    'subsample': 0.8, # 80% of data to grow trees and prevent overfitting\n",
    "    'colsample_bytree': 1, # % of features used\n",
    "    'eta': 0.08, \n",
    "    'max_depth': 10, \n",
    "    'seed': 42} # for reproducible results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGB with xgboost library (First simplified version just with train - Version 1)\n",
    "\n",
    "forecast_steps = 24\n",
    "\n",
    "XGB_metrics = pd.DataFrame()\n",
    "XGB_prediction = pd.DataFrame()\n",
    "\n",
    "df_forecast = pd.DataFrame()\n",
    "\n",
    "for i in mcs:\n",
    "    X_train = data_train[data_train['mc'] == i].copy()\n",
    "    y_train = np.log(X_train.v_flow_mean[X_train['mc'] == i] + 0.01)\n",
    "    X_test = data_test[data_test['mc'] == i].copy()\n",
    "    y_test = np.log(X_test.v_flow_mean[data_test['mc'] == i] + 0.01)\n",
    "    y_test_org = X_test.v_flow_mean[data_test['mc'] == i] + 0.01\n",
    "    \n",
    "    dtrain = xgb.DMatrix(X_train[predictors], y_train)\n",
    "    dtest = xgb.DMatrix(X_test[predictors], y_test)\n",
    "\n",
    "    watchlist = [(dtrain, 'train'), (dtest, 'test')]\n",
    "\n",
    "    print(\"\\n=====================================\")\n",
    "    print('MC = %s' % i)\n",
    "    print(\"=====================================\\n\")\n",
    "    \n",
    "    xgb_model = xgb.train(params, dtrain, 300, evals = watchlist, \n",
    "                          early_stopping_rounds = 100, feval = rmspe_xg, \n",
    "                          verbose_eval = False)\n",
    "\n",
    "#     Funcional sin tunning *********\n",
    "\n",
    "    yhat = xgb_model.predict(xgb.DMatrix(X_test[predictors]))\n",
    "    error = rmspe(X_test.v_flow_mean.values, np.exp(yhat))\n",
    "\n",
    "    print('First validation yelds RMSPE: {:.6f}'.format(error))\n",
    "    print(yhat)\n",
    "    \n",
    "    xgb.plot_importance(xgb_model)    \n",
    "    \n",
    "    # predictions to unseen data\n",
    "    unseen = xgb.DMatrix(X_test[predictors])\n",
    "    test_p = xgb_model.predict(unseen)\n",
    "    \n",
    "    temp_df = data_test[data_test['mc'] == i].copy()\n",
    "    temp_df = temp_df[['v_flow_mean', 'v_loss_cover', 'v_rainfall_total']]\n",
    "    temp_df.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    forecast = pd.DataFrame({'v_flow_mean_mean': np.exp(test_p)})\n",
    "    forecast = pd.concat([dates, forecast, temp_df], axis = 1)\n",
    "    forecast['mc'] = i\n",
    "    \n",
    "    print(forecast.head())\n",
    "    \n",
    "    forecast_errors = [temp_df.v_flow_mean.iloc[j] - forecast.v_flow_mean_mean.iloc[j] \n",
    "                       for j in range(forecast_steps)]\n",
    "    bias = sum(forecast_errors) * 1.0 / (forecast_steps)\n",
    "#     print('Bias : %f' % bias)\n",
    "\n",
    "    mae = skm.mean_absolute_error(temp_df.v_flow_mean, forecast.v_flow_mean_mean)\n",
    "#     print('MAE : %f' % mae)\n",
    "\n",
    "    mse = skm.mean_squared_error(temp_df.v_flow_mean, forecast.v_flow_mean_mean)\n",
    "    rmse = np.sqrt(mse)\n",
    "#     print('MSE : %f' % mse)\n",
    "#     print('RMSE : %f' % rmse) \n",
    "    \n",
    "    resultados = [i, bias, mae, mse, rmse]\n",
    "    resultados = pd.DataFrame([resultados], columns = ['mc', 'Bias', 'MAE', 'MSE', 'RMSE'])\n",
    "    \n",
    "    print(resultados.head())\n",
    "    print('===========================================================\\n')\n",
    "\n",
    "    forecast = forecast[['date', 'year', 'month', 'mc', 'v_flow_mean_mean', \n",
    "                         'v_flow_mean', 'v_loss_cover', 'v_rainfall_total']]\n",
    "    \n",
    "    XGB_metrics = pd.concat([XGB_metrics, resultados], axis = 0)\n",
    "    XGB_prediction = pd.concat([XGB_prediction, forecast], axis = 0)\n",
    "    \n",
    "#     print(y_test_org.head())\n",
    "\n",
    "#     ******************************************************************\n",
    "    \n",
    "#     Prueba ******\n",
    "    \n",
    "#     params_sk = {'max_depth': 10, \n",
    "#             'n_estimators': 100, # the same as num_rounds in xgboost\n",
    "#             'objective': 'reg:squarederror', \n",
    "#             'subsample': 0.8, \n",
    "#             'colsample_bytree': 0.85, \n",
    "#             'learning_rate': 0.025, \n",
    "#             'seed': 42}     \n",
    "\n",
    "#     skrg = XGBRegressor(**params_sk)\n",
    "\n",
    "#     skrg.fit(X_train[predictors], y_train, \n",
    "#              eval_set = [(X_train[predictors], y_train), (X_test[predictors], y_test)])\n",
    "\n",
    "#     results = skrg.evals_result()\n",
    "#     epochs = len(results['validation_0']['rmse'])\n",
    "#     x_axis = range(0, epochs)\n",
    "#     # plot AUC\n",
    "#     fig, ax = plt.subplots()\n",
    "#     ax.plot(x_axis, results['validation_0']['rmse'], label='Train')\n",
    "#     ax.plot(x_axis, results['validation_1']['rmse'], label='Test')\n",
    "#     ax.legend()\n",
    "#     plt.ylabel('RMSE')\n",
    "#     plt.title('XGBoost RMSE')\n",
    "#     plt.show()\n",
    "        \n",
    "    \n",
    "#     ******************************************************************\n",
    "    \n",
    "\n",
    "# Guardar en archivos\n",
    "\n",
    "XGB_metrics.to_csv('../model/XGB_results_v1.csv', index = False)\n",
    "XGB_metrics.head()\n",
    "\n",
    "XGB_prediction['v_flow_mean_mean'] = XGB_prediction['v_flow_mean_mean'].apply(lambda x: \n",
    "                                                                              0.01 if x <= 0 \n",
    "                                                                              else x)\n",
    "XGB_prediction.to_csv('../model/XGB_predictions_v1.csv', index = False)\n",
    "\n",
    "XGB_prediction.head()\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGB with xgboost library (First simplified version just with train - Version 1) - Prediction 2020 - 2021"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scenarios = pd.read_excel('../data/matrix/Esc_Predicciones_longitudinal.xlsx')\n",
    "\n",
    "scenarios['v_flow_mean'] = 0\n",
    "\n",
    "scenarios = scenarios[['date', 'mc', 'v_flow_mean', 'v_loss_cover', 'v_rainfall_total', \n",
    "                       'scenario']]\n",
    "\n",
    "scenarios.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates = pd.DataFrame(pd.date_range('2020-01-01','2021-12-31' , freq='1M') - \n",
    "             pd.offsets.MonthBegin(1))\n",
    "dates.columns = ['date']\n",
    "\n",
    "dates['year'] = pd.DatetimeIndex(dates['date']).year\n",
    "dates['month'] = pd.DatetimeIndex(dates['date']).month\n",
    "\n",
    "dates.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluation metric: rmspe\n",
    "# Root Mean Square Percentage Error\n",
    "# code chunk shared at Kaggle\n",
    "\n",
    "def rmspe(y, yhat):\n",
    "    return np.sqrt(np.mean((yhat / y-1) ** 2))\n",
    "\n",
    "def rmspe_xg(yhat, y):\n",
    "    y = np.expm1(y.get_label())\n",
    "    yhat = np.expm1(yhat)\n",
    "    return \"rmspe\", rmspe(y, yhat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# base parameters\n",
    "# params = {\n",
    "#     'booster': 'gbtree', \n",
    "#     'objective': 'reg:squarederror', # regression task\n",
    "#     'subsample': 0.8, # 80% of data to grow trees and prevent overfitting\n",
    "#     'colsample_bytree': 0.85, # % of features used\n",
    "#     'eta': 0.08, \n",
    "#     'max_depth': 10, \n",
    "#     'seed': 42} # for reproducible results\n",
    "\n",
    "params = {\n",
    "    'booster': 'gbtree', \n",
    "    'objective': 'reg:squarederror', # regression task\n",
    "    'subsample': 0.8, # 80% of data to grow trees and prevent overfitting\n",
    "    'colsample_bytree': 1, # % of features used\n",
    "    'eta': 0.08, \n",
    "    'max_depth': 10, \n",
    "    'seed': 42} # for reproducible results\n",
    "\n",
    "forecast_steps = 24\n",
    "predictors = ['v_loss_cover_porc', 'v_rainfall_total']\n",
    "escen = scenarios['scenario'].unique()\n",
    "\n",
    "XGB_prediction = pd.DataFrame()\n",
    "\n",
    "data_train_predict = macrodata.copy()\n",
    "data_train_predict['v_loss_cover_porc'] = data_train_predict['v_loss_cover'] * 100\n",
    "\n",
    "for j in escen:\n",
    "    \n",
    "    data_test_predict = scenarios[scenarios['scenario'] == j].copy()\n",
    "    data_test_predict['v_loss_cover_porc'] = data_test_predict['v_loss_cover'] * 100\n",
    "    \n",
    "    print('\\n==================================')\n",
    "    print('Escenario :', j)\n",
    "    print('==================================\\n')\n",
    "\n",
    "    for i in mcs:\n",
    "        X_train = data_train_predict[data_train_predict['mc'] == i].copy()\n",
    "        y_train = np.log(X_train.v_flow_mean[X_train['mc'] == i] + 0.01)\n",
    "        X_test = data_test_predict[data_test_predict['mc'] == i].copy()\n",
    "        y_test = np.log(X_test.v_flow_mean[data_test_predict['mc'] == i] + 0.01)\n",
    "        y_test_org = X_test.v_flow_mean[data_test_predict['mc'] == i] + 0.01\n",
    "\n",
    "        dtrain = xgb.DMatrix(X_train[predictors], y_train)\n",
    "        dtest = xgb.DMatrix(X_test[predictors], y_test)\n",
    "\n",
    "        watchlist = [(dtrain, 'train'), (dtest, 'test')]\n",
    "\n",
    "        xgb_model = xgb.train(params, dtrain, 300, evals = watchlist, \n",
    "                              early_stopping_rounds = 100, feval = rmspe_xg, \n",
    "                              verbose_eval = False)\n",
    "\n",
    "    #     Funcional sin tunning *********\n",
    "\n",
    "        yhat = xgb_model.predict(xgb.DMatrix(X_test[predictors]))\n",
    "#         error = rmspe(X_test.v_flow_mean.values, np.exp(yhat))\n",
    "\n",
    "#         print('First validation yelds RMSPE: {:.6f}'.format(error))\n",
    "#         print(yhat)\n",
    "\n",
    "#         xgb.plot_importance(xgb_model)    \n",
    "\n",
    "        # predictions to unseen data\n",
    "        unseen = xgb.DMatrix(X_test[predictors])\n",
    "        test_p = xgb_model.predict(unseen)\n",
    "\n",
    "        temp_df = data_test_predict[data_test_predict['mc'] == i].copy()\n",
    "        temp_df = temp_df[['v_flow_mean', 'v_loss_cover', 'v_rainfall_total']]\n",
    "        temp_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "        forecast = pd.DataFrame({'v_flow_mean_forecast': np.exp(test_p)})\n",
    "        forecast = pd.concat([dates, forecast, temp_df], axis = 1)\n",
    "        forecast['mc'] = i\n",
    "\n",
    "        print('\\n ===================================================== \\n')\n",
    "        \n",
    "        print(forecast.head())\n",
    "\n",
    "        forecast = forecast[['date', 'year', 'month', 'mc', 'v_flow_mean_forecast', \n",
    "                             'v_flow_mean', 'v_loss_cover', 'v_rainfall_total']]\n",
    "        \n",
    "        forecast['scenario'] = j\n",
    "        \n",
    "        print(j, i, forecast.shape, forecast.v_flow_mean_forecast.mean(), \n",
    "              np.exp(y_train).mean(), \n",
    "              forecast.v_loss_cover.mean(), forecast.v_rainfall_total.mean())\n",
    "\n",
    "        XGB_prediction = pd.concat([XGB_prediction, forecast], axis = 0)\n",
    "\n",
    "    XGB_prediction.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGB_prediction['v_flow_mean_forecast'] = XGB_prediction['v_flow_mean_forecast'].apply(lambda x: \n",
    "#                                                                               0.01 if x <= 0 \n",
    "#                                                                               else x)\n",
    "XGB_prediction.to_csv('../model/XGB_forecast_2020_2021.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.2 64-bit ('ds4a': conda)",
   "language": "python",
   "name": "python38264bitds4acondafc01702769ab4524a15cea8dad87138e"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
